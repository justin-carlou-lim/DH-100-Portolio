```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import spacy
import nltk
import gensim
from collections import Counter
from string import punctuation
```


```python
import nltk
from nltk.corpus import stopwords
```

# EDA #


```python
# Import Dataframes
s_script = pd.read_csv('simpsons_data/simpsons_script_lines.csv')
s_char = pd.read_csv('simpsons_data/simpsons_characters.csv')
s_ep = pd.read_csv('simpsons_data/simpsons_episodes.csv') 
s_loc = pd.read_csv('simpsons_data/simpsons_locations.csv')
```

    /Users/justincarloulim/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (4,5,6) have mixed types.Specify dtype option on import or set low_memory=False.
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,



```python
list(s_loc.columns)
```




    ['id', 'name', 'normalized_name']




```python
# Create a dataframe that merges both script and the character information.
# Now, we have the gender of each character, as well as their names, the timestamp
# of their speaking lines, and their character id.

script = s_script[['episode_id', 
                   'timestamp_in_ms',
                   'raw_location_text',
                   'raw_character_text', 
                   'spoken_words', 
                   'word_count']].rename(columns = {'raw_character_text': 'name', 
                                                    'spoken_words': 'line'})
chars = s_char.dropna().rename(columns = {'id': 'char_id'})
script_table = script.merge(chars)
script_table = script_table[['episode_id', 'normalized_name', 'char_id', 'gender' , 'line', 'timestamp_in_ms', 'raw_location_text']]
script_table.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>episode_id</th>
      <th>normalized_name</th>
      <th>char_id</th>
      <th>gender</th>
      <th>line</th>
      <th>timestamp_in_ms</th>
      <th>raw_location_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>32</td>
      <td>miss hoover</td>
      <td>464</td>
      <td>f</td>
      <td>No, actually, it was a little of both. Sometim...</td>
      <td>848000</td>
      <td>Springfield Elementary School</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>miss hoover</td>
      <td>464</td>
      <td>f</td>
      <td>I don't know. Although I'd sure like to talk t...</td>
      <td>856000</td>
      <td>Springfield Elementary School</td>
    </tr>
    <tr>
      <th>2</th>
      <td>49</td>
      <td>miss hoover</td>
      <td>464</td>
      <td>f</td>
      <td>Thank you, Ralph. Very graphic. Lisa Simpson? ...</td>
      <td>784000</td>
      <td>Springfield Elementary School</td>
    </tr>
    <tr>
      <th>3</th>
      <td>49</td>
      <td>miss hoover</td>
      <td>464</td>
      <td>f</td>
      <td>Dear God!</td>
      <td>803000</td>
      <td>Springfield Elementary School</td>
    </tr>
    <tr>
      <th>4</th>
      <td>51</td>
      <td>miss hoover</td>
      <td>464</td>
      <td>f</td>
      <td>I question the educational value of this assem...</td>
      <td>226000</td>
      <td>Springfield Elementary School</td>
    </tr>
  </tbody>
</table>
</div>



# Gender Distribution Over Time #


```python
## Testing Idea
test_32 = script_table[['episode_id', 'gender', 'normalized_name']]
test_32 = test_32[test_32['episode_id'] == 32]
test_counts = test_32.groupby('gender').count()
#test_counts
```


```python
## Create counts table
gdt = script_table[['episode_id', 'gender', 'normalized_name']].sort_values(['episode_id'])
counts_gdt = gdt.groupby(['episode_id', 'gender']).count()
counts_gdt

m_dist = []
f_dist = []

n = len(counts_gdt)
for i in range(n):
    if i % 2 == 0:
        f_dist.append(counts_gdt.iloc[i][0])
    else:
        m_dist.append(counts_gdt.iloc[i][0])

        
gdt_table = pd.DataFrame({'female_dist': f_dist, 'male_dist': m_dist}).reset_index()
gdt_table['total'] = [a + b for a, b in zip(f_dist, m_dist)]
gdt_table['female_dist'] = gdt_table['female_dist']/ gdt_table['total']
gdt_table['male_dist'] = gdt_table['male_dist']/ gdt_table['total']
gdt_table['total'] = gdt_table['total']/ gdt_table['total']
gdt_table.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>female_dist</th>
      <th>male_dist</th>
      <th>total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.278997</td>
      <td>0.721003</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.233202</td>
      <td>0.766798</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.257143</td>
      <td>0.742857</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.291045</td>
      <td>0.708955</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.188755</td>
      <td>0.811245</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
## Graph Distributions

def create_avgs(tbl, n):
    f = []
    m = []

    i = 0
    while i < len(tbl):
        if i + n > len(tbl):
            f.append(np.mean(tbl['female_dist'][i:]))
            m.append(np.mean(tbl['male_dist'][i:]))
        else:
            f.append(np.mean(tbl['female_dist'][i:i+n]))
            m.append(np.mean(tbl['male_dist'][i:i+n]))
        i += n
        
    return f, m


f_avgs, m_avgs = create_avgs(gdt_table, 200)
plt.figure(figsize= (6,4))
plt.plot(f_avgs, label= 'Female dist')
plt.plot(m_avgs, label= 'Male dist')
plt.ylabel('Percentage of Speaking Lines Over Time')
plt.xlabel('Time')
plt.title('Percentage of Speaking Lines Over Time for each gender (AVG 200)')
plt.legend()
plt.show();
```


    
![png](output_9_0.png)
    


# Network Analysis Model #


```python
srt_st = script_table.sort_values(by = ['episode_id', 'timestamp_in_ms'])
srt_st[srt_st['episode_id'] == 1]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>episode_id</th>
      <th>normalized_name</th>
      <th>char_id</th>
      <th>gender</th>
      <th>line</th>
      <th>timestamp_in_ms</th>
      <th>raw_location_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>72629</th>
      <td>1</td>
      <td>marge simpson</td>
      <td>1</td>
      <td>f</td>
      <td>Ooo, careful, Homer.</td>
      <td>8000.0</td>
      <td>Car</td>
    </tr>
    <tr>
      <th>57449</th>
      <td>1</td>
      <td>homer simpson</td>
      <td>2</td>
      <td>m</td>
      <td>There's no time to be careful.</td>
      <td>10000.0</td>
      <td>Car</td>
    </tr>
    <tr>
      <th>57450</th>
      <td>1</td>
      <td>homer simpson</td>
      <td>2</td>
      <td>m</td>
      <td>We're late.</td>
      <td>10000.0</td>
      <td>Car</td>
    </tr>
    <tr>
      <th>72630</th>
      <td>1</td>
      <td>marge simpson</td>
      <td>1</td>
      <td>f</td>
      <td>Sorry, Excuse us. Pardon me...</td>
      <td>24000.0</td>
      <td>Auditorium</td>
    </tr>
    <tr>
      <th>57451</th>
      <td>1</td>
      <td>homer simpson</td>
      <td>2</td>
      <td>m</td>
      <td>Hey, Norman. How's it going? So you got dragge...</td>
      <td>26000.0</td>
      <td>Auditorium</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>72670</th>
      <td>1</td>
      <td>marge simpson</td>
      <td>1</td>
      <td>f</td>
      <td>Take it, Homer!</td>
      <td>1358000.0</td>
      <td>Simpson Home</td>
    </tr>
    <tr>
      <th>57565</th>
      <td>1</td>
      <td>homer simpson</td>
      <td>2</td>
      <td>m</td>
      <td>RUDOLPH WITH YOUR NOSE... OVER HERE... / SO YO...</td>
      <td>1360000.0</td>
      <td>Simpson Home</td>
    </tr>
    <tr>
      <th>72671</th>
      <td>1</td>
      <td>marge simpson</td>
      <td>1</td>
      <td>f</td>
      <td>Oh, Homer.</td>
      <td>1365000.0</td>
      <td>Simpson Home</td>
    </tr>
    <tr>
      <th>11043</th>
      <td>1</td>
      <td>lisa simpson</td>
      <td>9</td>
      <td>f</td>
      <td>THEN ALL THE REINDEER LOVED HIM / AS THEY SHOU...</td>
      <td>1366000.0</td>
      <td>Simpson Home</td>
    </tr>
    <tr>
      <th>25401</th>
      <td>1</td>
      <td>bart simpson</td>
      <td>8</td>
      <td>m</td>
      <td>Like Attila the Hun!</td>
      <td>1379000.0</td>
      <td>Simpson Home</td>
    </tr>
  </tbody>
</table>
<p>319 rows Ã— 7 columns</p>
</div>



# Topic Model #


```python
spacy.load('en_core_web_sm')
from spacy.lang.en import English
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer

parser = English()
en_stop = set(nltk.corpus.stopwords.words('english'))
```


```python
def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        elif type(token) != str:
            continue
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens

def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is None:
        return word
    else:
        return lemma
    
def get_lemma2(word):
    return WordNetLemmatizer().lemmatize(word)

def prepare_text_for_lda(text):
    tokens = text.lower()
    for char in punctuation:
        tokens = tokens.replace(char, '')
    tokens = tokens.split()
    return tokens
```


```python
#script_table['line']
```


```python
# Lower and remove punctuation from each line, and put
# Into a 2d list
text_data = []
for line in script_table['line']:
    if type(line) != str:
        continue
    tokens = prepare_text_for_lda(line)
    text_data.append(tokens)
```




    111314




```python
quarter_index = int(len(text_data)/4)
text_data_quarter = text_data[: quarter_index]
```


```python
from gensim import corpora
dictionary = corpora.Dictionary(text_data)
corpus = [dictionary.doc2bow(text) for text in text_data_quarter]
import pickle
pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')
```


```python
import gensim
NUM_TOPICS = 5
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)
```

    (0, '0.045*"i" + 0.043*"the" + 0.032*"a" + 0.027*"to"')
    (1, '0.026*"okay" + 0.014*"gotta" + 0.009*"kid" + 0.007*"thank"')
    (2, '0.036*"oh" + 0.027*"no" + 0.020*"my" + 0.020*"im"')
    (3, '0.061*"you" + 0.031*"what" + 0.028*"do" + 0.027*"hey"')
    (4, '0.022*"uh" + 0.008*"a" + 0.008*"cool" + 0.007*"um"')
